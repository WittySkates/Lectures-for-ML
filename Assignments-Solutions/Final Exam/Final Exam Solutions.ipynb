{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Exam Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "**Consider the following three decision boundaries estimated using the same training data but with three different learning algorithms: a random forest with 100 learners, a k-Nearest Neighbors (kNN) and a Multi-Layer Perceptron (MLP) with a single hidden layer and non-linear activation function. Which method estimated each decision boundary? Clearly provide the reasons for your selection.**\n",
    "\n",
    "<div><img src=\"DecisionBoundary.png\", width=\"800\"><!div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision boundary 1 was estimated estimated using k-NN. k-NN is highly prone to overfitting and it is indicative by the little red islands in a sea of blues depicted in decision boundary 1.\n",
    "\n",
    "Decision boundary 2 was estimated using a single hidden layer MLP. This is clearly visible as a single hidden layer MLP will create a smooth boundary.\n",
    "\n",
    "Decision boundary 3 was estimated using random forests. The boundary gives it away from its horizontal and vertical edges as they correspond to splits in decision tree nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "**Consider the following learning curves obtained by training a Multi-Layer Perceptron (MLP) using mini-batch, batch and online learning. Which learning curve corresponds to each type of learning? Justify your answer.**\n",
    "\n",
    "<div><img src=\"LearningCurves.png\", width=\"600\"><!div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning curve 3 corresponds to online learning. We observe that in online learning the initial learning epochs converge quicker but the become unstable as the learning rate becomes to large to reach the local minima.\n",
    "\n",
    "Learning Curve 2 corresponds to batch learning. Using batch learning we will make an update based on the average response from all data samples. The learning will therefore be the slowest to converge but it will result in a smaller misadjustment (difference between converge error and minima).\n",
    "\n",
    "Learning Curve 1 corresponds to mini-batch learnin. The speed of learning and solution convergence have been considered: mini-batch learning is slightly slower than online but its performance is much more stable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "\n",
    "**Consider the linear activation function**\n",
    "\n",
    "$$\\phi(x) = x$$\n",
    "\n",
    "**and the hard-limit activation function**\n",
    "\n",
    "$$\\phi(x) = \\begin{cases}0 & x\\leq 0 \\\\ 1 & x>0\\end{cases}$$\n",
    "\n",
    "**Which of the following functions can be exactly represented by a neural network with one hidden layer which uses linear and/or hard-limit activation functions? For each case, justify your answer.**\n",
    "\n",
    "**Case 1: polynomials of degree one.**\n",
    "\n",
    "**Case 2: hinge loss of the form $h(x)=\\max(1-x,0)$.**\n",
    "\n",
    "**Case 3: polynomials of degree two.**\n",
    "\n",
    "**Case 4: piecewise constant functions.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Case 1:** Yes. Polynomial of degree one are lines. These type of decision boundaries can be represented by one hidden layer with either linear or hard-limit activation function.\n",
    "\n",
    "**Case 2:** No. Hinge loss is a non-linear function are therefore cannot be \n",
    "\n",
    "**Case 3:** No. Polynomial of degree two cannot be represented with these activation functions.\n",
    "\n",
    "**Case 4:** Yes. Piecewise constant functions can be represented with any of these functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4\n",
    "\n",
    "**This question addresses Convolutional Neural Networks (CNNs). From the options below, select all that are true of CNNs for image analysis:**\n",
    "\n",
    "**A: Filters in earlier layers tend to include edge detectors**\n",
    "\n",
    "**B: Pooling layers reduce the spatial resolution of the image**\n",
    "\n",
    "**C: They have more parameters than fully connected networks with the same number of layers and the same numbers of neurons in each layer**\n",
    "\n",
    "**D: A CNN can be trained for unsupervised learning tasks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Options A, B and D are correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5\n",
    "\n",
    "**From the images below, which show 1) too large, 2) too small, and 3) correctly selected learning rate after 3 iterations? Justify your answer.**\n",
    "\n",
    "<div><img src=\"LearningRates.png\", width=\"600\"><!div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image A uses a too large learning rate. The weight update makes drastic move making the gradient change direction.\n",
    "\n",
    "Image B uses correctly selected learning rate. The weight updates are movinf smoothly in the direction of the minima.\n",
    "\n",
    "Image C uses a too small learning rate. The weights move in the direction of the minima but much slower than in image B."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6\n",
    "\n",
    "**What does momentum assist with in training artificial neural networks?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Momentum assists with speed of learning. In neural network, the learning rate parameter needs to be crucially defined to ensure stability and convergence. Too large of learning rate will cause instability and sometimes diverge. Too small learning rate will cause stability but it will take too long to converge. A momentum term can be added to updating rule of the network parameters to speed learning without the need of increasing the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 7\n",
    "\n",
    "**Inspired by ensemble learning, what is the approach commonly used today to regularize deep neural networks? Explain why it works.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble learning techniques include bagging and boosting. Both of these techniques are derived from a data point-of-view: bagging generate several Bootstrap samples of the data set and fits them all to a fixed network architecture; boosting weights data samples based on how difficult they are to make a classification prediction.\n",
    "\n",
    "Today, a common regularization technique is dropout. Dropout is inspired by ensemble learning as at every epoch, it will random deactivate units to ensure that the system doesn't overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 8\n",
    "\n",
    "**You want to train a neural network to drive a car. Your training data consists of grayscale $64\\times 64$ pixel images. The training labels include the human driver’s steering wheel angle in degrees and the human driver’s speed in miles per hour. Your neural network consists of an input layer with $64\\times 64 = 4,096$ units, a hidden layer with  units, and an output layer with 2 units (one for steering angle, one for speed). You use the ReLU activation function for the hidden units and no activation function for the outputs.**\n",
    "\n",
    "**Answer the following questions:**\n",
    "\n",
    "**a) Calculate the number of parameters (weights) in this network. You can leave your answer as an expression. Be sure to account for the bias terms.**\n",
    "\n",
    "**b) You train your network with the cost function**\n",
    "\n",
    "$$J = \\frac{1}{2}\\sum_{i=1}^N e_i^2$$\n",
    "\n",
    "**where we define the error as $e=y-t$, $y$ is the output value and  is the target label value.**\n",
    "\n",
    "**Derive the update equation for each weight in the output layer.**\n",
    "\n",
    "**(Note that in class we considered the error as $e=t-y$).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 1**\n",
    "\n",
    "There are $4097 \\times 2048 + 2049 \\times 2$ total number of parameters (including both weights and biases).\n",
    "\n",
    "**Part 2**\n",
    "\n",
    "Let $y_i = \\sum_{j=1}^M w_{ij} z_j$ be the units in the output layer and $z_j = \\phi\\left(\\sum_{k=1}^D w_{kj}x_k\\right)$ be units in the hidden layer where $\\phi(\\bullet)$ is the ReLu function.\n",
    "\n",
    "The update equation for each weight in the output layer is:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial w_{ij}} = \\frac{\\partial J}{\\partial e_i}\\frac{\\partial e_i}{\\partial y_i}\\frac{\\partial y_i}{\\partial z_j}\\frac{\\partial z_j}{\\partial w_{ij}} = \\frac{1}{2}2e \\times 1\\times 1 \\times z_j = (y-t) z_j$$\n",
    "\n",
    "$$w_{ij}^{(t+1)} = w_{ij}^{(t)} - \\eta (y-t) z_j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 9\n",
    "\n",
    "**Consider the following two-dimensional, two-class data set plotted in the figure below. Design a neural network that would obtain 100% classification accuracy on this data set. Be sure to define the network architecture and all parameters precisely (give exact numbers). Use a hard-limit activation function as defined in the following equation:**\n",
    "\n",
    "$$\\phi(x)=\\begin{cases}0 & x\\leq 0 \\\\ 1 & x>0\\end{cases}$$\n",
    "\n",
    "**Show and explain your work.**\n",
    "\n",
    "<div><img src=\"Network.png\", width=\"600\"><!div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div><img src=\"Problem9solution.png\", width=\"800\"><!div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 10\n",
    "\n",
    "**Consider the following perceptron:**\n",
    "\n",
    "<div><img src=\"Perceptron.png\", width=\"300\"><!div>\n",
    "\n",
    "**Recall that the perceptron uses the activation function:**\n",
    "    \n",
    "$$\\phi(x)=\\begin{cases}-1 & x\\leq 0 \\\\ 1 & x>0\\end{cases}$$\n",
    "    \n",
    "**And the cost function is:**\n",
    "\n",
    "$$E_p(\\mathbf{w},b) = -\\sum_{m\\in\\mathcal{M}}(\\mathbf{w}^T\\mathbf{x}_n + b)^T t_n$$\n",
    "\n",
    "**where $\\mathcal{M}$ is the set of all misclassified points.**\n",
    "    \n",
    "**The update equations for the weights and bias term are:**\n",
    "  \n",
    "\\begin{eqnarray}\n",
    "\\mathbf{w}^{(t+1)} &\\leftarrow & \\mathbf{w}^{(t)} - \\eta \\frac{\\partial E_p(\\mathbf{w},b)}{\\partial \\mathbf{w}} = \\mathbf{w}^{(t)} + \\eta \\mathbf{x}_n t_n\\\\\n",
    "b^{(t+1)} &\\leftarrow & b^{(t)} - \\eta\\frac{\\partial E_p(\\mathbf{w},b)}{\\partial b} = b^{(t)} + \\eta t_n\n",
    "\\end{eqnarray}\n",
    "    \n",
    "**Suppose you have the following 5 data samples $(x,y)$ and their corresponding labels $t$:**\n",
    "    \n",
    "**$(x_1,y_1)=(1,0)$ with $t_1=1$**\n",
    "    \n",
    "**$(x_2,y_2)=(4,2)$ with $t_2=1$**\n",
    "\n",
    "**$(x_3,y_3)=(0,-1)$ with $t_3=-1$**\n",
    "\n",
    "**$(x_4,y_4)=(-1,-1)$ with $t_4=-1$**\n",
    "\n",
    "**$(x_5,y_5)=(-2,1)$ with $t_5=-1$**\n",
    "\n",
    "**What is the smallest value for the learning rate $\\eta$ such that the updated network will result in zero misclassified points using only one iteration?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div><img src=\"Problem10Asolution.png\", width=\"800\"><!div>\n",
    "    \n",
    "<div><img src=\"Problem10Bsolution.png\", width=\"600\"><!div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 11 - extra credit\n",
    "\n",
    "**Consider the problem of *vanishing gradients* with training networks with many layers (e.g. deep neural networks). The gradient diminishes dramatically as it is propagated backward through the network and the error may be too small by the time it reaches layers close to the input.**\n",
    "\n",
    "**Can we combat this problem by choosing an appropriate activation function?  If yes, which activation function, why and what's the catch? If no, explain why not.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, ReLU activation function. ReLU is an unbouded function which may cause problems in tasks such as classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
